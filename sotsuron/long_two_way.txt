[11-21 12:01:32 MainThread @logger.py:242] Argv: collect_two_way.py
/home/student/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() > 0
[11-21 12:01:34 MainThread @__init__.py:44] PARL detects two backend frameworks: paddle, torch. Use paddle by default.
[11-21 12:01:34 MainThread @__init__.py:45] To use torch as backend, `export PARL_BACKEND=torch` before running the scripts.
384 76 6571 10339
 1と２
analist: 1, step: 2, baseline: 6, promising: 4
/usr/lib/python3/dist-packages/apport/report.py:13: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import fnmatch, glob, traceback, errno, sys, atexit, imp, stat
Traceback (most recent call last):
  File "collect_two_way.py", line 60, in <module>
    bfcount, bfdcount, sfcount, sfdcount, size = dataset.collect_two_ways_cache(sample_system, analist, step=step, baseline=baseline, promising=promising, mode="focus")
AttributeError: 'DatasetManager' object has no attribute 'collect_two_ways_cache'
[11-22 17:23:08 MainThread @logger.py:242] Argv: collect_two_way.py
/home/student/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() > 0
[11-22 17:23:10 MainThread @__init__.py:44] PARL detects two backend frameworks: paddle, torch. Use paddle by default.
[11-22 17:23:10 MainThread @__init__.py:45] To use torch as backend, `export PARL_BACKEND=torch` before running the scripts.
384 76 6571 10339
 1と２
analist: 1, step: 2, baseline: 6, promising: 4
result: 1 0 0 0 0 0
result: 2 0 0 0 0 0
result: 3 0 0 0 0 0
result: 4 0 0 0 0 0
[11-22 17:36:07 MainThread @logger.py:242] Argv: collect_two_way.py
/home/student/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() > 0
[11-22 17:36:09 MainThread @__init__.py:44] PARL detects two backend frameworks: paddle, torch. Use paddle by default.
[11-22 17:36:09 MainThread @__init__.py:45] To use torch as backend, `export PARL_BACKEND=torch` before running the scripts.
/home/student/PARL/benchmark/torch/AlphaZero/sotsuron/connect4_game.py:232: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.
  return board.tostring()
384 76 6571 10339
 1と２
analist: 1, step: 2, baseline: 6, promising: 4
0
result: 1 0.7005208333333334 0.6790364583333334 0.484375 0.4114583333333333 384
1
result: 2 0.7105263157894737 0.6973684210526315 0.631578947368421 0.6217105263157895 76
2
result: 3 0.7301780550905493 0.6978770354588343 0.5154466595647542 0.5108050525034241 6571
3
/usr/lib/python3/dist-packages/apport/report.py:13: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import fnmatch, glob, traceback, errno, sys, atexit, imp, stat
Traceback (most recent call last):
  File "collect_two_way.py", line 61, in <module>
    bfcount, bfdcount, sfcount, sfdcount, size = dataset.collect_two_ways_cache(sample_system, analist, step=step, baseline=baseline, promising=promising, mode="focus")
  File "/home/student/PARL/benchmark/torch/AlphaZero/sotsuron/feature.py", line 1925, in collect_two_ways_cache
    ave_bfrate += bfcount
TypeError: unsupported operand type(s) for +=: 'int' and 'list'
[11-22 19:10:48 MainThread @logger.py:242] Argv: collect_two_way.py
/home/student/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:107: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)
  return torch._C._cuda_getDeviceCount() > 0
[11-22 19:10:50 MainThread @__init__.py:44] PARL detects two backend frameworks: paddle, torch. Use paddle by default.
[11-22 19:10:50 MainThread @__init__.py:45] To use torch as backend, `export PARL_BACKEND=torch` before running the scripts.
/home/student/PARL/benchmark/torch/AlphaZero/sotsuron/connect4_game.py:232: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.
  return board.tostring()
384 76 6571 10339
 1と２
analist: 1, step: 2, baseline: 6, promising: 4
result: 4 0.6894283779862656 0.7006238514363091 0.5873875616597349 0.6256649579262985 10339
